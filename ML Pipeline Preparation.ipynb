{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'stopwords'])\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterMessages.db')\n",
    "df = pd.read_sql_table('Messages', engine)\n",
    "df.head()\n",
    "\n",
    "X = df.message.values\n",
    "Y = df.drop(['id', 'message', 'original', 'genre'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Strips the characters that aren't alphanumeric and then processes the tokens\n",
    "\n",
    "    Args:\n",
    "    text: string containing message\n",
    "\n",
    "    Returns:\n",
    "    stemmed and cleaned tokens\n",
    "    \"\"\"\n",
    "    # strip non-alphanumeric chars and lowercase the text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize to words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lammatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "\n",
    "#     clean_tokens = []\n",
    "#     for tok in tokens:\n",
    "#         clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "#         clean_tokens.append(clean_tok)\n",
    "        \n",
    "    clean_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 42)\n",
    "\n",
    "np.random.seed(33)\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_metrics(actual, predicted, col_names):\n",
    "    \"\"\"Calculate evaluation metrics for ML model\n",
    "    \n",
    "    Args:\n",
    "    actual: array. Array containing actual labels.\n",
    "    predicted: array. Array containing predicted labels.\n",
    "    col_names: list of strings. List containing names for each of the predicted fields.\n",
    "       \n",
    "    Returns:\n",
    "    metrics_df: dataframe. Dataframe containing the accuracy, precision, recall \n",
    "    and f1 score for a given set of actual and predicted labels.\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Calculate evaluation metrics for each set of labels\n",
    "    for i in range(len(col_names)):\n",
    "        accuracy = accuracy_score(actual[:, i], predicted[:, i])\n",
    "        precision = precision_score(actual[:, i], predicted[:, i], average='macro')\n",
    "        recall = recall_score(actual[:, i], predicted[:, i], average='macro')\n",
    "        f1 = f1_score(actual[:, i], predicted[:, i], average='macro')\n",
    "        \n",
    "        metrics.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    # Create dataframe containing metrics\n",
    "    metrics = np.array(metrics)\n",
    "    metrics_df = pd.DataFrame(data = metrics, index = col_names, columns = ['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "      \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.990845   0.985462  0.969525  0.977334\n",
      "request                 0.987133   0.990795  0.963801  0.976676\n",
      "offer                   0.998627   0.999311  0.841176  0.905250\n",
      "aid_related             0.983979   0.985507  0.981660  0.983457\n",
      "medical_help            0.987946   0.992835  0.924089  0.955389\n",
      "medical_products        0.991456   0.995546  0.913313  0.950306\n",
      "search_and_rescue       0.994660   0.997266  0.907080  0.947410\n",
      "security                0.994965   0.997449  0.860563  0.917707\n",
      "military                0.995830   0.997016  0.938610  0.965859\n",
      "child_alone             1.000000   1.000000  1.000000  1.000000\n",
      "water                   0.994863   0.997272  0.959729  0.977652\n",
      "food                    0.994202   0.996547  0.974030  0.984944\n",
      "shelter                 0.992422   0.995304  0.957532  0.975498\n",
      "clothing                0.998423   0.999200  0.949511  0.973013\n",
      "money                   0.995931   0.997924  0.915074  0.952556\n",
      "missing_people          0.997559   0.998768  0.893333  0.939682\n",
      "refugees                0.993744   0.994099  0.909012  0.947265\n",
      "death                   0.993795   0.996770  0.931996  0.961897\n",
      "other_aid               0.979860   0.987668  0.924137  0.952829\n",
      "infrastructure_related  0.985302   0.992255  0.888331  0.933244\n",
      "transport               0.991252   0.994798  0.904762  0.944828\n",
      "buildings               0.992218   0.995932  0.924257  0.956983\n",
      "electricity             0.996592   0.998268  0.912987  0.951480\n",
      "tools                   0.998474   0.999234  0.870690  0.925359\n",
      "hospitals               0.996796   0.998384  0.861233  0.918628\n",
      "shops                   0.998067   0.999031  0.802083  0.876138\n",
      "aid_centers             0.996389   0.998180  0.844298  0.906881\n",
      "other_infrastructure    0.989930   0.992646  0.887593  0.933260\n",
      "weather_related         0.988862   0.991062  0.981391  0.986080\n",
      "floods                  0.991862   0.995601  0.951100  0.972084\n",
      "storm                   0.994202   0.996299  0.969476  0.982422\n",
      "fire                    0.998220   0.999102  0.918981  0.955470\n",
      "earthquake              0.995931   0.995024  0.981192  0.987985\n",
      "cold                    0.996796   0.996872  0.920283  0.955251\n",
      "other_weather           0.989676   0.994608  0.902498  0.943271\n",
      "direct_report           0.982453   0.987986  0.955840  0.970937\n"
     ]
    }
   ],
   "source": [
    "Y_train_pred = pipeline.predict(X_train)\n",
    "col_names = list(Y.columns.values)\n",
    "\n",
    "print(get_eval_metrics(np.array(Y_train), Y_train_pred, col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.799359   0.600069  0.550573  0.569991\n",
      "request                 0.885261   0.841399  0.709205  0.750346\n",
      "offer                   0.994965   0.497482  0.500000  0.498738\n",
      "aid_related             0.754196   0.750301  0.732381  0.737325\n",
      "medical_help            0.920201   0.744009  0.546978  0.564620\n",
      "medical_products        0.949649   0.828401  0.534079  0.550531\n",
      "search_and_rescue       0.975587   0.723744  0.524454  0.539268\n",
      "security                0.981996   0.616221  0.504077  0.503791\n",
      "military                0.969637   0.742356  0.550926  0.579404\n",
      "child_alone             1.000000   1.000000  1.000000  1.000000\n",
      "water                   0.945377   0.876807  0.592949  0.638885\n",
      "food                    0.918676   0.886469  0.668610  0.723367\n",
      "shelter                 0.934544   0.875389  0.666545  0.722537\n",
      "clothing                0.987183   0.902967  0.591527  0.646766\n",
      "money                   0.979707   0.740293  0.522089  0.536249\n",
      "missing_people          0.989167   0.994582  0.513699  0.523943\n",
      "refugees                0.966585   0.650540  0.508671  0.509120\n",
      "death                   0.960177   0.871596  0.583056  0.628266\n",
      "other_aid               0.867562   0.672520  0.516828  0.501652\n",
      "infrastructure_related  0.936680   0.468626  0.499674  0.483652\n",
      "transport               0.955447   0.795808  0.541704  0.564143\n",
      "buildings               0.954684   0.898723  0.549054  0.576993\n",
      "electricity             0.977876   0.864008  0.510126  0.514272\n",
      "tools                   0.993439   0.496720  0.500000  0.498354\n",
      "hospitals               0.991456   0.495728  0.500000  0.497855\n",
      "shops                   0.996338   0.498169  0.500000  0.499083\n",
      "aid_centers             0.987489   0.493820  0.499923  0.496852\n",
      "other_infrastructure    0.956668   0.692902  0.504981  0.499267\n",
      "weather_related         0.859933   0.841775  0.787425  0.808192\n",
      "floods                  0.941410   0.879843  0.665279  0.722903\n",
      "storm                   0.931950   0.834466  0.703418  0.748869\n",
      "fire                    0.990235   0.828599  0.530149  0.553101\n",
      "earthquake              0.969332   0.931441  0.873187  0.899687\n",
      "cold                    0.980165   0.885096  0.552880  0.588730\n",
      "other_weather           0.949802   0.763552  0.532962  0.548414\n",
      "direct_report           0.837504   0.780031  0.623147  0.650351\n"
     ]
    }
   ],
   "source": [
    "Y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "eval_metrics0 = get_eval_metrics(np.array(Y_test), Y_test_pred, col_names)\n",
    "print(eval_metrics0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance metric for use in grid search scoring object\n",
    "def performance_metric(y_true, y_pred):\n",
    "    \"\"\"Calculate median F1 score for all of the output classifiers\n",
    "    \n",
    "    Args:\n",
    "    y_true: array. Array containing actual labels.\n",
    "    y_pred: array. Array containing predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "    score: float. Median F1 score for all of the output classifiers\n",
    "    \"\"\"\n",
    "    f1_list = []\n",
    "    for i in range(np.shape(y_pred)[1]):\n",
    "        f1 = f1_score(np.array(y_true)[:, i], y_pred[:, i], average='macro')\n",
    "        f1_list.append(f1)\n",
    "        \n",
    "    score = np.median(f1_list)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.5435348882803395, total=  53.4s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.552873531107866, total=  53.8s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.5675979886936737, total=  53.9s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  3.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.5710661898739202, total=  41.8s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  4.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.5778949647171693, total=  41.6s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.5839662753264463, total=  41.6s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  6.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.5425798104250306, total=  54.1s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  7.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.5367502756387188, total=  54.9s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  8.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.5552138251596097, total=  55.4s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  9.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.5742938879906486, total=  41.5s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.5663271670132417, total=  41.2s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.5907377937567246, total=  41.5s\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.5604777984807566, total= 1.9min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.5521595192551846, total= 1.9min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.5627242561003942, total= 1.9min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.589568231235396, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.5917907228042247, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.6047279421552912, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.5514120983352437, total= 1.9min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.5489458994273224, total= 1.9min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.5684916011397871, total= 1.9min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.5822019196160005, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.5817339892208523, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=2, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.5924712233260914, total= 1.3min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.5668362530967643, total=  45.9s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.5598507907933197, total=  46.1s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.5697060377355871, total=  46.4s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.5995422147492819, total=  38.7s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.5823389177456182, total=  38.3s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.5965385465168143, total=  38.7s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.5586265854473683, total=  46.0s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.5828848454141001, total=  46.0s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.5514609039531213, total=  46.0s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.5885864805490078, total=  37.6s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.6044992699000642, total=  37.5s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.6008253926430411, total=  37.6s\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.5428145436061306, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.5479795412968131, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.5579657472692503, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.5879697343459062, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.5784177801461178, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.609995372060199, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.5484279020965079, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.5412756003092125, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.5496928902335952, total= 1.5min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.5936702716735804, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.5783803111670373, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=5, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.6035778157228011, total= 1.2min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.5605364155185029, total=  42.3s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.5592467649790126, total=  42.3s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=1, score=0.5463920440999672, total=  42.1s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.5919573198532759, total=  36.5s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.5984460216831118, total=  36.5s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__min_df=5, score=0.6042073621618664, total=  36.5s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.5472393324940742, total=  41.6s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.555045413845666, total=  42.2s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=1, score=0.5619187205108, total=  42.1s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.6051918242093679, total=  36.1s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.5886029075553565, total=  35.5s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__min_df=5, score=0.5946136550476517, total=  35.5s\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.5601773771459722, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.5491093127609623, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=1, score=0.5487537232982822, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.5785685894238893, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.6039659633216644, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__min_df=5, score=0.585696043069031, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.5344764741270566, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.5409850600481452, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=1, score=0.5515215879063868, total= 1.4min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.5817992917733406, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.5850845728001787, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5 \n",
      "[CV]  clf__estimator__min_samples_split=10, clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__min_df=5, score=0.581783190424182, total= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed: 95.6min finished\n"
     ]
    }
   ],
   "source": [
    "# Create grid search object\n",
    "\n",
    "parameters = {'vect__min_df': [1, 5],\n",
    "              'tfidf__use_idf':[True, False],\n",
    "              'clf__estimator__n_estimators':[10, 25], \n",
    "              'clf__estimator__min_samples_split':[2, 5, 10]}\n",
    "\n",
    "scorer = make_scorer(performance_metric)\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, scoring = scorer, verbose = 10)\n",
    "\n",
    "# Find best parameters\n",
    "np.random.seed(81)\n",
    "tuned_model = cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  46.44882234,   34.85331853,   47.56287551,   34.64383698,\n",
       "         102.21323061,   72.0846231 ,  102.98815521,   71.10837523,\n",
       "          38.91847706,   31.77689393,   38.82274715,   30.82738908,\n",
       "          81.96438734,   63.86672791,   80.7208724 ,   61.17300733,\n",
       "          35.0429608 ,   29.839516  ,   34.86358802,   29.05485765,\n",
       "          71.92614102,   59.49111438,   71.54929868,   57.1304423 ]),\n",
       " 'std_fit_time': array([ 0.2120114 ,  0.07386094,  0.4632251 ,  0.12783927,  0.33434777,\n",
       "         0.22132051,  0.9003926 ,  0.13499275,  0.183641  ,  0.13943674,\n",
       "         0.01541135,  0.03904305,  0.13858673,  0.30396897,  1.17186422,\n",
       "         0.20420184,  0.09822844,  0.01953783,  0.25627104,  0.26128957,\n",
       "         0.33766267,  0.36409989,  0.16324261,  0.09638975]),\n",
       " 'mean_score_time': array([ 7.25142256,  6.80136704,  7.24962942,  6.75237187,  9.90628815,\n",
       "         8.74117668,  9.76895118,  8.59326061,  7.22366238,  6.79770343,\n",
       "         7.18712862,  6.74429297,  9.83472617,  8.68884476,  9.68498087,\n",
       "         8.52988044,  7.16979806,  6.67338943,  7.11745667,  6.65534536,\n",
       "         9.63127907,  8.54512429,  9.58674256,  8.50494766]),\n",
       " 'std_score_time': array([ 0.01845141,  0.00886547,  0.06065145,  0.02066819,  0.06604136,\n",
       "         0.02707609,  0.01271797,  0.02001315,  0.01649967,  0.02166371,\n",
       "         0.02286337,  0.02835273,  0.04938179,  0.01324203,  0.05631199,\n",
       "         0.03299251,  0.03878438,  0.01007036,  0.00845163,  0.02709416,\n",
       "         0.01936223,  0.01218962,  0.01430673,  0.00396565]),\n",
       " 'param_clf__estimator__min_samples_split': masked_array(data = [2 2 2 2 2 2 2 2 5 5 5 5 5 5 5 5 10 10 10 10 10 10 10 10],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_clf__estimator__n_estimators': masked_array(data = [10 10 10 10 25 25 25 25 10 10 10 10 25 25 25 25 10 10 10 10 25 25 25 25],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_tfidf__use_idf': masked_array(data = [True True False False True True False False True True False False True\n",
       "  True False False True True False False True True False False],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__min_df': masked_array(data = [1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5 1 5],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 2,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 5,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 10,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': True,\n",
       "   'vect__min_df': 5},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 1},\n",
       "  {'clf__estimator__min_samples_split': 10,\n",
       "   'clf__estimator__n_estimators': 25,\n",
       "   'tfidf__use_idf': False,\n",
       "   'vect__min_df': 5}],\n",
       " 'split0_test_score': array([ 0.54353489,  0.57106619,  0.54257981,  0.57429389,  0.5604778 ,\n",
       "         0.58956823,  0.5514121 ,  0.58220192,  0.56683625,  0.59954221,\n",
       "         0.55862659,  0.58858648,  0.54281454,  0.58796973,  0.5484279 ,\n",
       "         0.59367027,  0.56053642,  0.59195732,  0.54723933,  0.60519182,\n",
       "         0.56017738,  0.57856859,  0.53447647,  0.58179929]),\n",
       " 'split1_test_score': array([ 0.55287353,  0.57789496,  0.53675028,  0.56632717,  0.55215952,\n",
       "         0.59179072,  0.5489459 ,  0.58173399,  0.55985079,  0.58233892,\n",
       "         0.58288485,  0.60449927,  0.54797954,  0.57841778,  0.5412756 ,\n",
       "         0.57838031,  0.55924676,  0.59844602,  0.55504541,  0.58860291,\n",
       "         0.54910931,  0.60396596,  0.54098506,  0.58508457]),\n",
       " 'split2_test_score': array([ 0.56759799,  0.58396628,  0.55521383,  0.59073779,  0.56272426,\n",
       "         0.60472794,  0.5684916 ,  0.59247122,  0.56970604,  0.59653855,\n",
       "         0.5514609 ,  0.60082539,  0.55796575,  0.60999537,  0.54969289,\n",
       "         0.60357782,  0.54639204,  0.60420736,  0.56191872,  0.59461366,\n",
       "         0.54875372,  0.58569604,  0.55152159,  0.58178319]),\n",
       " 'mean_test_score': array([ 0.5546688 ,  0.57764248,  0.54484797,  0.57711962,  0.55845386,\n",
       "         0.5953623 ,  0.5562832 ,  0.58546904,  0.56546436,  0.59280656,\n",
       "         0.56432411,  0.59797038,  0.54958661,  0.59212763,  0.54646546,\n",
       "         0.59187613,  0.55539174,  0.59820357,  0.55473449,  0.59613613,\n",
       "         0.55268014,  0.5894102 ,  0.54232771,  0.58288902]),\n",
       " 'std_test_score': array([ 0.0099054 ,  0.00526946,  0.00770645,  0.01016393,  0.00454428,\n",
       "         0.00668438,  0.00869116,  0.00495497,  0.00413868,  0.00750263,\n",
       "         0.0134465 ,  0.00680282,  0.00628897,  0.01322251,  0.00370595,\n",
       "         0.01036477,  0.00638549,  0.005004  ,  0.00599687,  0.00685743,\n",
       "         0.00530334,  0.01069588,  0.0070231 ,  0.00155251]),\n",
       " 'rank_test_score': array([19, 11, 23, 12, 15,  4, 16,  9, 13,  5, 14,  2, 21,  6, 22,  7, 17,\n",
       "         1, 18,  3, 20,  8, 24, 10], dtype=int32),\n",
       " 'split0_train_score': array([ 0.95517076,  0.95573055,  0.95363462,  0.95762254,  0.9920251 ,\n",
       "         0.99172507,  0.99238769,  0.99122509,  0.92852985,  0.92067263,\n",
       "         0.9193705 ,  0.91789831,  0.95413868,  0.93266631,  0.94894474,\n",
       "         0.93293847,  0.90142362,  0.88142088,  0.88919153,  0.87542745,\n",
       "         0.91980497,  0.89855989,  0.89994923,  0.88349834]),\n",
       " 'split1_train_score': array([ 0.95624077,  0.95016481,  0.95009341,  0.95589451,  0.99083312,\n",
       "         0.99032066,  0.99183377,  0.99166744,  0.92340471,  0.92071882,\n",
       "         0.92203854,  0.91459169,  0.94732219,  0.93654981,  0.94419209,\n",
       "         0.92113974,  0.90029972,  0.86997827,  0.88786202,  0.87572451,\n",
       "         0.9146032 ,  0.89553064,  0.9023255 ,  0.88844815]),\n",
       " 'split2_train_score': array([ 0.95096207,  0.95490078,  0.95149306,  0.9579806 ,  0.99118954,\n",
       "         0.99086727,  0.99022129,  0.99020047,  0.9318271 ,  0.91420242,\n",
       "         0.921203  ,  0.9057086 ,  0.95332958,  0.93734989,  0.94679001,\n",
       "         0.92157726,  0.89943686,  0.88508149,  0.89739375,  0.86195792,\n",
       "         0.91494169,  0.88766544,  0.90477341,  0.89559479]),\n",
       " 'mean_train_score': array([ 0.95412453,  0.95359872,  0.95174036,  0.95716588,  0.99134925,\n",
       "         0.990971  ,  0.99148092,  0.991031  ,  0.92792055,  0.91853129,\n",
       "         0.92087068,  0.91273287,  0.95159681,  0.935522  ,  0.94664228,\n",
       "         0.92521849,  0.90038673,  0.87882688,  0.89148243,  0.87103663,\n",
       "         0.91644995,  0.89391865,  0.90234938,  0.88918042]),\n",
       " 'std_train_score': array([ 0.00227847,  0.00245165,  0.00145623,  0.00091081,  0.00049956,\n",
       "         0.00057802,  0.00091895,  0.00061441,  0.00346532,  0.00306103,\n",
       "         0.00111428,  0.00514708,  0.00304061,  0.00204553,  0.00194307,\n",
       "         0.00546177,  0.00081342,  0.00643291,  0.00421502,  0.00642076,\n",
       "         0.00237638,  0.00459138,  0.00196953,  0.00496543])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get results of grid search\n",
    "tuned_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__min_samples_split': 10,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__min_df': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for best mean test score\n",
    "tuned_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.799969   0.571464  0.545884  0.551565\n",
      "request                 0.885413   0.834017  0.717177  0.755781\n",
      "offer                   0.994965   0.497482  0.500000  0.498738\n",
      "aid_related             0.761062   0.752578  0.752280  0.752427\n",
      "medical_help            0.922185   0.753445  0.582970  0.615163\n",
      "medical_products        0.955600   0.884627  0.599002  0.649363\n",
      "search_and_rescue       0.977266   0.805430  0.574377  0.616057\n",
      "security                0.981080   0.541213  0.503611  0.503161\n",
      "military                0.970095   0.752836  0.582639  0.621158\n",
      "child_alone             1.000000   1.000000  1.000000  1.000000\n",
      "water                   0.961703   0.896094  0.749922  0.803980\n",
      "food                    0.941562   0.881732  0.808481  0.839863\n",
      "shelter                 0.932103   0.845850  0.669090  0.720390\n",
      "clothing                0.987946   0.897630  0.627086  0.690755\n",
      "money                   0.980012   0.776158  0.529608  0.549368\n",
      "missing_people          0.989167   0.994582  0.513699  0.523943\n",
      "refugees                0.968874   0.776362  0.590736  0.632891\n",
      "death                   0.963534   0.858691  0.644147  0.701241\n",
      "other_aid               0.867104   0.675212  0.531780  0.529826\n",
      "infrastructure_related  0.936222   0.602212  0.503971  0.492910\n",
      "transport               0.954379   0.742342  0.566267  0.597912\n",
      "buildings               0.955752   0.848594  0.577503  0.618699\n",
      "electricity             0.978334   0.878194  0.523653  0.539390\n",
      "tools                   0.993439   0.496720  0.500000  0.498354\n",
      "hospitals               0.991456   0.495728  0.500000  0.497855\n",
      "shops                   0.996338   0.498169  0.500000  0.499083\n",
      "aid_centers             0.987641   0.493821  0.500000  0.496891\n",
      "other_infrastructure    0.954989   0.514155  0.500730  0.491853\n",
      "weather_related         0.871529   0.844197  0.821987  0.831972\n",
      "floods                  0.954837   0.900897  0.760624  0.812989\n",
      "storm                   0.942020   0.838068  0.797310  0.815996\n",
      "fire                    0.990082   0.795190  0.522573  0.539761\n",
      "earthquake              0.969332   0.923584  0.882351  0.901646\n",
      "cold                    0.981538   0.834593  0.629880  0.685058\n",
      "other_weather           0.949802   0.752540  0.545671  0.568487\n",
      "direct_report           0.845743   0.778740  0.663771  0.694560\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for test set\n",
    "tuned_pred_test = tuned_model.predict(X_test)\n",
    "\n",
    "eval_metrics1 = get_eval_metrics(np.array(Y_test), tuned_pred_test, col_names)\n",
    "\n",
    "print(eval_metrics1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.944173</td>\n",
       "      <td>0.757346</td>\n",
       "      <td>0.588627</td>\n",
       "      <td>0.607653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.057562</td>\n",
       "      <td>0.152386</td>\n",
       "      <td>0.115889</td>\n",
       "      <td>0.124496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.754196</td>\n",
       "      <td>0.468626</td>\n",
       "      <td>0.499674</td>\n",
       "      <td>0.483652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.933895</td>\n",
       "      <td>0.667025</td>\n",
       "      <td>0.509762</td>\n",
       "      <td>0.507788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.958422</td>\n",
       "      <td>0.787919</td>\n",
       "      <td>0.544341</td>\n",
       "      <td>0.564381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.983293</td>\n",
       "      <td>0.875743</td>\n",
       "      <td>0.633680</td>\n",
       "      <td>0.668397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  36.000000  36.000000  36.000000  36.000000\n",
       "mean    0.944173   0.757346   0.588627   0.607653\n",
       "std     0.057562   0.152386   0.115889   0.124496\n",
       "min     0.754196   0.468626   0.499674   0.483652\n",
       "25%     0.933895   0.667025   0.509762   0.507788\n",
       "50%     0.958422   0.787919   0.544341   0.564381\n",
       "75%     0.983293   0.875743   0.633680   0.668397\n",
       "max     1.000000   1.000000   1.000000   1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get summary stats for first model\n",
    "eval_metrics0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.947030</td>\n",
       "      <td>0.756476</td>\n",
       "      <td>0.619966</td>\n",
       "      <td>0.642752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.055970</td>\n",
       "      <td>0.152866</td>\n",
       "      <td>0.127701</td>\n",
       "      <td>0.135216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.761062</td>\n",
       "      <td>0.493821</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.491853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.940227</td>\n",
       "      <td>0.656962</td>\n",
       "      <td>0.520355</td>\n",
       "      <td>0.528355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.962618</td>\n",
       "      <td>0.786965</td>\n",
       "      <td>0.580071</td>\n",
       "      <td>0.617378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.983064</td>\n",
       "      <td>0.863566</td>\n",
       "      <td>0.681112</td>\n",
       "      <td>0.728399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Precision     Recall         F1\n",
       "count  36.000000  36.000000  36.000000  36.000000\n",
       "mean    0.947030   0.756476   0.619966   0.642752\n",
       "std     0.055970   0.152866   0.127701   0.135216\n",
       "min     0.761062   0.493821   0.500000   0.491853\n",
       "25%     0.940227   0.656962   0.520355   0.528355\n",
       "50%     0.962618   0.786965   0.580071   0.617378\n",
       "75%     0.983064   0.863566   0.681112   0.728399\n",
       "max     1.000000   1.000000   1.000000   1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get summary stats for tuned model\n",
    "eval_metrics1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle best model\n",
    "pickle.dump(tuned_model, open('DisasterMessagesModel.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
